{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb2266ad-b712-4987-8a7f-eab700a0842d",
   "metadata": {},
   "source": [
    "# Consensus-Driven FedAvg Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c1376c-9ab7-4632-aab3-264cc6e4d4db",
   "metadata": {},
   "source": [
    "# 1. Local Model Training\n",
    "## Each CLient trains a local model on it's private data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0af5f6f-ba7d-4383-a867-b35e44b30ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        return self.fc2\n",
    "\n",
    "def train_client(model, data_loader, epochs=1, lr=0.01):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for inputs, labels in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return {name: param.data.clone().tolist() for name, param in model.state_dict().items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547077e5-c28d-4060-b359-0c81473d080c",
   "metadata": {},
   "source": [
    "# 2. Peer-to-Peer Communication\n",
    "## Clients exchange their model updates via a peer-to-peer communication layer.\n",
    "### Example Communication Layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a2b6479-acab-482c-9d48-28415fb552c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from threading import Thread\n",
    "\n",
    "app = Flask(__name__)\n",
    "peer_updates = []\n",
    "\n",
    "@app.route('/send_update', methods=['POST'])\n",
    "def receive_update():\n",
    "    update = request.json\n",
    "    peer_updates.append(update)\n",
    "    return jsonify({\"message\": \"Update received\"}), 200\n",
    "\n",
    "@app.route('/get_updates', methods=['GET'])\n",
    "def get_updates():\n",
    "    return jsonify(peer_updates), 200\n",
    "\n",
    "def run_peer_server(port):\n",
    "    app.run(port=port)\n",
    "\n",
    "# Start a peer server\n",
    "server_thread = Thread(target=run_peer_server, args=(5000,))\n",
    "server_thread.start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32912d4d-cecb-4d7a-b7a7-625b5e407131",
   "metadata": {},
   "source": [
    "# 3. Consensus Mechanism\n",
    "## A consensus protocol ensures that all nodes agree on the set of updates to use for aggregation. For simplicity, we’ll use a basic voting mechanism where the majority determines valid updates.\n",
    "### Voting Consensus Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30a8d9b3-1493-4824-b89e-5a11a14112a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def consensus_voting(peer_updates, threshold=0.6):\n",
    "    # Count votes for each update\n",
    "    votes = {}\n",
    "    for update in peer_updates:\n",
    "        update_hash = str(update)  # Simplified unique identifier\n",
    "        votes[update_hash] = votes.get(update_hash, 0) + 1\n",
    "\n",
    "    # Select updates with sufficient votes\n",
    "    valid_updates = [update for update, count in votes.items() if count / len(peer_updates) >= threshold]\n",
    "    return valid_updates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fa1e05-34a6-4b94-9362-c04e2df832d4",
   "metadata": {},
   "source": [
    "# 4. Federated Averaging (FedAvg)\n",
    "## Aggregate the selected updates into a global model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ccce8d2-61f6-4a43-b1be-eb7e550bc7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def federated_averaging(valid_updates):\n",
    "    aggregated_model = {}\n",
    "    num_updates = len(valid_updates)\n",
    "\n",
    "    # Average updates\n",
    "    for key in valid_updates[0]:\n",
    "        aggregated_model[key] = sum([torch.tensor(update[key]) for update in valid_updates]) / num_updates\n",
    "\n",
    "    return aggregated_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4526d8-7151-4774-a075-d6cd53220737",
   "metadata": {},
   "source": [
    "# 5. CDFA Workflow\n",
    "## Integrate all components into a full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd3eea99-4e57-4ddf-ae09-d0afa8c3770a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consensus_driven_federated_averaging(num_rounds=5, num_clients=10, epochs=1):\n",
    "    # Initialize global model\n",
    "    global_model = SimpleNN()\n",
    "    clients_data = []\n",
    "\n",
    "    # Simulate data for clients\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "    dataset = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "    clients_data = random_split(dataset, [6000] * num_clients)\n",
    "\n",
    "    for round_num in range(num_rounds):\n",
    "        print(f\"Round {round_num + 1}/{num_rounds}\")\n",
    "        peer_updates = []\n",
    "\n",
    "        # Simulate clients training locally\n",
    "        for client_id in range(num_clients):\n",
    "            client_loader = DataLoader(clients_data[client_id], batch_size=32, shuffle=True)\n",
    "            local_model = SimpleNN()\n",
    "            local_model.load_state_dict(global_model.state_dict())\n",
    "            client_update = train_client(local_model, client_loader, epochs=epochs)\n",
    "            peer_updates.append(client_update)\n",
    "\n",
    "        # Consensus mechanism to validate updates\n",
    "        valid_updates = consensus_voting(peer_updates)\n",
    "\n",
    "        # Aggregate valid updates\n",
    "        aggregated_params = federated_averaging(valid_updates)\n",
    "\n",
    "        # Update global model\n",
    "        global_model.load_state_dict(aggregated_params)\n",
    "\n",
    "    print(\"Consensus-Driven Federated Learning Complete\")\n",
    "    return global_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aff04cd-49ce-479a-991f-280d03b9ad57",
   "metadata": {},
   "source": [
    "# 6. Evaluate the Global Model\n",
    "## Evaluate the aggregated model using a test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fad7038-97fa-4fe7-97d1-c9c3d66d225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Global Model Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412ca13e-090e-4e88-bc7b-72db0eb51c3c",
   "metadata": {},
   "source": [
    "# 7. Run the Full Workflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aceefae4-54ff-471c-8ea0-d4026e94b189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [05:26<00:00, 30.4kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 119kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.07MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 1.52MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Round 1/5\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not Linear",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     global_model \u001b[38;5;241m=\u001b[39m consensus_driven_federated_averaging()\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Load test data\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([transforms\u001b[38;5;241m.\u001b[39mToTensor(), transforms\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.5\u001b[39m,), (\u001b[38;5;241m0.5\u001b[39m,))])\n",
      "Cell \u001b[1;32mIn[5], line 20\u001b[0m, in \u001b[0;36mconsensus_driven_federated_averaging\u001b[1;34m(num_rounds, num_clients, epochs)\u001b[0m\n\u001b[0;32m     18\u001b[0m     local_model \u001b[38;5;241m=\u001b[39m SimpleNN()\n\u001b[0;32m     19\u001b[0m     local_model\u001b[38;5;241m.\u001b[39mload_state_dict(global_model\u001b[38;5;241m.\u001b[39mstate_dict())\n\u001b[1;32m---> 20\u001b[0m     client_update \u001b[38;5;241m=\u001b[39m train_client(local_model, client_loader, epochs\u001b[38;5;241m=\u001b[39mepochs)\n\u001b[0;32m     21\u001b[0m     peer_updates\u001b[38;5;241m.\u001b[39mappend(client_update)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Consensus mechanism to validate updates\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 28\u001b[0m, in \u001b[0;36mtrain_client\u001b[1;34m(model, data_loader, epochs, lr)\u001b[0m\n\u001b[0;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     27\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m---> 28\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     29\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mC:\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mC:\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1293\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcross_entropy(\n\u001b[0;32m   1294\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1295\u001b[0m         target,\n\u001b[0;32m   1296\u001b[0m         weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m   1297\u001b[0m         ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index,\n\u001b[0;32m   1298\u001b[0m         reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction,\n\u001b[0;32m   1299\u001b[0m         label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing,\n\u001b[0;32m   1300\u001b[0m     )\n",
      "File \u001b[1;32mC:\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:3479\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3478\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\n\u001b[0;32m   3480\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   3481\u001b[0m     target,\n\u001b[0;32m   3482\u001b[0m     weight,\n\u001b[0;32m   3483\u001b[0m     _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction),\n\u001b[0;32m   3484\u001b[0m     ignore_index,\n\u001b[0;32m   3485\u001b[0m     label_smoothing,\n\u001b[0;32m   3486\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not Linear"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    global_model = consensus_driven_federated_averaging()\n",
    "\n",
    "    # Load test data\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "    test_dataset = datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Evaluate the global model\n",
    "    evaluate_model(global_model, test_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
